name: Initial Repository Setup

on:
  create:
  workflow_dispatch:
    inputs:
      reset_structure:
        description: 'Reset directory structure'
        required: false
        default: false
        type: boolean
      install_sample_data:
        description: 'Install sample data for testing'
        required: false
        default: true
        type: boolean

jobs:
  setup:
    runs-on: ubuntu-latest
    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # Step 3: Create complete directory structure
      - name: Create directory structure
        run: |
          echo "🏗️ Setting up repository structure..."
          directories=(
            "data"
            "output"
            "output/backups"
            "output/reports"
            "output/charts"
            "logs"
            ".github/sample_data"
            ".github/scripts"
            "docs"
            "tests"
          )
          for dir in "${directories[@]}"; do
            mkdir -p "$dir"
            echo "✅ Created: $dir"
          done
          # Create .gitkeep files
          echo "📝 Creating .gitkeep files..."
          echo "# Place your lme_copper_historical_data.csv here" > data/.gitkeep
          echo "# Analysis output files" > output/.gitkeep
          echo "# Timestamped backup files" > output/backups/.gitkeep
          echo "# Analysis reports" > output/reports/.gitkeep
          echo "# Generated charts" > output/charts/.gitkeep
          echo "# Log files" > logs/.gitkeep
          echo "# Sample data for testing" > .github/sample_data/.gitkeep
          echo "# Utility scripts" > .github/scripts/.gitkeep
          echo "# Documentation" > docs/.gitkeep
          echo "# Test files" > tests/.gitkeep
          echo "✅ Directory structure created successfully!"
          tree -a -I '.git'

      # Step 4: Generate sample data (optional)
      - name: Generate sample data
        if: ${{ github.event.inputs.install_sample_data == 'true' || github.event_name == 'create' }}
        run: |
          echo "📊 Generating sample data for testing..."
          python3 - <<'PYTHON'
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
np.random.seed(42)
end_date = datetime.now()
start_date = end_date - timedelta(days=365*3)
dates = pd.date_range(start=start_date, end=end_date, freq='B')
base_price = 9000
trend = np.linspace(0, 500, len(dates))
seasonal = 300 * np.sin(2 * np.pi * np.arange(len(dates)) / 252)
noise = np.random.normal(0, 200, len(dates))
prices = base_price + trend + seasonal + noise
prices = np.maximum(prices, 5000)
df = pd.DataFrame({
  'date': dates,
  'lme_copper_cash_settlement': prices,
  'lme_copper_3_month': prices + np.random.normal(50, 20, len(dates)),
  'lme_copper_stock': np.random.randint(100000, 200000, len(dates))
})
df['date'] = df['date'].dt.strftime('%-m/%-d/%Y')
Path(".github/sample_data").mkdir(parents=True, exist_ok=True)
df.to_csv('.github/sample_data/lme_copper_historical_data.csv', index=False)
print(f'✅ Generated sample data with {len(df)} rows')
df.head(100).to_csv('.github/sample_data/test_data_small.csv', index=False)
print('✅ Generated small test dataset (100 rows)')
PYTHON
          # Copy sample data to data directory if empty
          if [ ! -f "data/lme_copper_historical_data.csv" ]; then
            cp .github/sample_data/lme_copper_historical_data.csv data/
            echo "📁 Sample data copied to data/ directory"
          fi

      # Step 5: Create utility scripts
      - name: Create utility scripts
        run: |
          echo "📝 Creating utility scripts..."
          cat > .github/scripts/validate_data.py <<'EOF'
#!/usr/bin/env python3
"""Validate LME copper data CSV file"""
import pandas as pd
import sys
def validate_csv(filepath):
    try:
        df = pd.read_csv(filepath)
        required_cols = ['date', 'lme_copper_cash_settlement']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"❌ Missing required columns: {missing_cols}")
            return False
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        if df['date'].isna().any():
            print(f"❌ Invalid date values found")
            return False
        if (df['lme_copper_cash_settlement'] < 0).any():
            print(f"❌ Negative prices found")
            return False
        print(f"✅ Data validation passed!")
        print(f" Rows: {len(df)}")
        print(f" Date range: {df['date'].min()} to {df['date'].max()}")
        print(f" Price range: ${df['lme_copper_cash_settlement'].min():.2f} to ${df['lme_copper_cash_settlement'].max():.2f}")
        return True
    except Exception as e:
        print(f"❌ Validation error: {e}")
        return False
if __name__ == "__main__":
    filepath = sys.argv[1] if len(sys.argv) > 1 else "data/lme_copper_historical_data.csv"
    sys.exit(0 if validate_csv(filepath) else 1)
EOF
          chmod +x .github/scripts/validate_data.py
          echo "✅ Created data validation script"

          cat > .github/scripts/quick_analysis.py <<'EOF'
#!/usr/bin/env python3
"""Quick analysis of LME copper data"""
import pandas as pd
import json
from datetime import datetime
def quick_analysis(filepath):
    df = pd.read_csv(filepath)
    df['date'] = pd.to_datetime(df['date'])
    stats = {
        "data_info": {
            "total_rows": len(df),
            "date_range": f"{df['date'].min().date()} to {df['date'].max().date()}",
            "columns": list(df.columns)
        },
        "price_stats": {
            "mean": float(df['lme_copper_cash_settlement'].mean()),
            "median": float(df['lme_copper_cash_settlement'].median()),
            "std": float(df['lme_copper_cash_settlement'].std()),
            "min": float(df['lme_copper_cash_settlement'].min()),
            "max": float(df['lme_copper_cash_settlement'].max())
        },
        "recent_data": {
            "last_price": float(df.iloc[0]['lme_copper_cash_settlement']),
            "last_date": df.iloc[0]['date'].strftime('%Y-%m-%d'),
            "last_week_avg": float(df.head(5)['lme_copper_cash_settlement'].mean())
        }
    }
    print(json.dumps(stats, indent=2))
    return stats
if __name__ == "__main__":
    import sys
    filepath = sys.argv[1] if len(sys.argv) > 1 else "data/lme_copper_historical_data.csv"
    quick_analysis(filepath)
EOF
          chmod +x .github/scripts/quick_analysis.py
          echo "✅ Created quick analysis script"

      # Step 6: Create or update README with setup status
      - name: Update README with setup status
        run: |
          python3 - <<'PYTHON'
import os
from datetime import datetime
if not os.path.exists('README.md'):
    readme_content = '''# LME Copper Analysis Tool 🔧📊

A comprehensive Python-based analysis tool for London Metal Exchange (LME) Copper historical data.

## 🚀 Quick Start

This repository has been automatically set up with GitHub Actions!

1. **Add your data**: Place your `lme_copper_historical_data.csv` file in the `data/` directory

2. **Run analysis**: Use the Actions tab to trigger the analysis workflow

3. **View results**: Open `copper_dashboard.html` after the workflow completes

## 📁 Repository Structure

✅ All directories have been created automatically!

'''
else:
    with open('README.md', 'r') as f:
        readme_content = f.read()
setup_status = f'''
## ✅ Setup Status

**Last Setup:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}

**Status:** Repository structure initialized

**Sample Data:** {'Installed' if os.path.exists('.github/sample_data/lme_copper_historical_data.csv') else 'Not installed'}
'''
if '## ✅ Setup Status' not in readme_content:
    if '# ' in readme_content:
        idx = readme_content.find('\n')
        readme_content = readme_content[:idx] + '\n' + setup_status + readme_content[idx:]
    else:
        readme_content += '\n' + setup_status
else:
    parts = readme_content.split('## ✅ Setup Status')
    readme_content = parts[0] + setup_status + (parts[1][parts[1].find('\n', 1):] if len(parts) > 1 else '')
with open('README.md', 'w') as f:
    f.write(readme_content)
print('✅ README updated with setup status')
PYTHON

      # Step 7: Create GitHub Actions secrets documentation
      - name: Create secrets documentation
        run: |
          cat > docs/SECRETS_SETUP.md <<'EOF'
# GitHub Secrets Setup Guide

## Required Secrets for Full Functionality

### 1. Email Notifications (Optional)
If you want to receive email notifications when analysis completes:

1. Go to Settings → Secrets and variables → Actions
2. Add the following secrets:
   - `EMAIL_ADDRESS`: Your Gmail address
   - `EMAIL_PASSWORD`: Gmail app password (not regular password)
   - `NOTIFICATION_EMAIL`: Where to send notifications

**Note:** For Gmail, you need to:
1. Enable 2-factor authentication
2. Generate an app-specific password
3. Use the app password as `EMAIL_PASSWORD`

### 2. Advanced Features (Optional)
For future enhancements, you might add:
- `SLACK_WEBHOOK`: For Slack notifications
- `TEAMS_WEBHOOK`: For Microsoft Teams notifications
- `DATABASE_URL`: For storing results in a database

## How to Add Secrets
1. Navigate to your repository on GitHub
2. Click on "Settings" tab
3. Select "Secrets and variables" → "Actions"
4. Click "New repository secret"
5. Enter the secret name and value
6. Click "Add secret"

## Security Notes
- Never commit secrets to the repository
- Secrets are encrypted and only visible to workflows
- Rotate secrets regularly for security
EOF

          echo "✅ Created secrets documentation"

      # Step 8: Create test file
      - name: Create test file
        run: |
          cat > tests/test_analysis.py <<'EOF'
#!/usr/bin/env python3
"""Basic tests for copper analysis"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import unittest
from pathlib import Path

# Assuming copper_analysis.py is in the root dir and defines CopperLMEAnalyzer for real tests
class TestCopperAnalysis(unittest.TestCase):
    def setUp(self):
        """Set up test fixtures"""
        # Dummy placeholder: Replace with actual analyzer if available
        self.test_data_path = Path('.github/sample_data/test_data_small.csv')
    def test_data_loading(self):
        """Test data loading functionality"""
        self.assertTrue(self.test_data_path.exists())
    def test_directory_structure(self):
        """Test that required directories exist"""
        required_dirs = ['data', 'output', 'output/backups', 'logs']
        for dir_path in required_dirs:
            self.assertTrue(Path(dir_path).exists(), f"Directory {dir_path} does not exist")

if __name__ == '__main__':
    unittest.main()
EOF
          chmod +x tests/test_analysis.py
          echo "✅ Created test file"

      # Step 9: Install dependencies
      - name: Install Python dependencies
        run: |
          echo "📦 Installing Python dependencies..."
          if [ ! -f "requirements.txt" ]; then
            cat > requirements.txt <<'EOF'
# Core dependencies
pandas>=1.5.0
numpy>=1.23.0
scipy>=1.9.0
schedule>=1.1.0
# Additional dependencies for enhanced features
openpyxl>=3.0.0 # Excel support
tabulate>=0.9.0 # Table formatting
matplotlib>=3.6.0 # Plotting
seaborn>=0.12.0 # Statistical plots
EOF
          echo "✅ Created requirements.txt"
          fi
          pip install -r requirements.txt
          echo "✅ Dependencies installed"

      # Step 10: Run validation
      - name: Validate setup
        run: |
          echo "🔍 Validating setup..."
          for dir in data output output/backups output/reports logs; do
            if [ -d "$dir" ]; then
              echo "✅ Directory exists: $dir"
            else
              echo "❌ Missing directory: $dir"
              exit 1
            fi
          done
          # Check for data file
          if [ -f "data/lme_copper_historical_data.csv" ]; then
            echo "✅ Data file found"
            python .github/scripts/validate_data.py
          else
            echo "⚠️ No data file found (sample data available in .github/sample_data/)"
          fi
          # Check Python environment
          echo ""
          echo "Python environment:"
          python --version
          echo ""
          echo "Installed packages:"
          pip list | grep -E "pandas|numpy|scipy|schedule"
          echo ""
          echo "🎉 Setup validation complete!"

      # Step 11: Create workflow documentation
      - name: Create workflow documentation
        run: |
          cat > docs/WORKFLOWS.md <<'EOF'
# GitHub Actions Workflows Documentation

## Available Workflows

### 1. Initial Repository Setup (`setup.yml`)
**Trigger:**
- Automatically when repository is created
- Manually via Actions tab

**What it does:**
- Creates complete directory structure
- Generates sample data for testing
- Creates utility scripts
- Validates setup

**Manual Options:**
- `reset_structure`: Recreate directory structure
- `install_sample_data`: Install sample test data

### 2. LME Copper Analysis (`copper_analysis.yml`)
**Trigger:**
- Manual with custom options
- Daily at 7:30 PM UTC
- Weekly on Mondays at 9 AM UTC
- Monthly on 1st day at midnight UTC
- On push to main branch
- On pull requests

**Manual Options:**
- `analysis_type`: full, last_year, last_6_months, last_3_months, custom
- `start_date`: For custom analysis
- `end_date`: For custom analysis
- `output_format`: json, json_and_csv, json_and_excel, all_formats
- `create_backup`: Create timestamped backup
- `send_notification`: Send email (requires secrets)

**Outputs:**
- Analysis results in multiple formats
- Interactive dashboard update
- Automated backups
- Summary reports

## How to Trigger Workflows Manually
1. Go to the "Actions" tab in your repository
2. Select the workflow you want to run
3. Click "Run workflow"
4. Fill in the options
5. Click "Run workflow" button

## Workflow Artifacts
Each workflow run produces artifacts that can be downloaded:
- `analysis-results-{run-number}`: Complete analysis outputs
- `analysis-logs-{run-number}`: Execution logs

## Scheduled Runs
The analysis workflow runs automatically:
- **Daily**: 7:30 PM UTC (adjust in workflow file for your timezone)
- **Weekly**: Monday 9 AM UTC (weekly summary)
- **Monthly**: 1st day midnight UTC (monthly report + release)
To modify schedule, edit the `cron` expressions in `.github/workflows/copper_analysis.yml`

## Notifications
Set up email notifications by adding these secrets:
1. `EMAIL_ADDRESS`: Your email
2. `EMAIL_PASSWORD`: App password
3. `NOTIFICATION_EMAIL`: Recipient email
See `docs/SECRETS_SETUP.md` for detailed instructions.
EOF
          echo "✅ Created workflow documentation"

      # Step 12: Commit all changes (if running on create)
      - name: Commit changes
        if: github.event_name == 'create'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          # Only commit if there are changes
          if ! git diff --staged --quiet; then
            git commit -m "🚀 Initial repository setup complete

- Created directory structure
- Added sample data for testing
- Created utility scripts
- Added documentation
- Validated setup

[skip ci]"
            git push
          fi

      # Step 13: Create setup completion issue (on create, or if reset_structure is true)
      - name: Create setup completion issue
        if: github.event_name == 'create' || github.event.inputs.reset_structure == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '✅ Repository Setup Complete!',
              body: `## 🎉 Your LME Copper Analysis repository is ready!

### ✅ Setup Checklist:

- [x] Directory structure created
- [x] Sample data generated
- [x] Utility scripts added
- [x] Documentation created
- [x] Workflows configured

### 🚀 Next Steps:

1. **Add your data**:
   - Place your \`lme_copper_historical_data.csv\` file in the \`data/\` directory
   - Or use the sample data already provided for testing

2. **Run your first analysis**:
   - Go to the [Actions tab](../../actions)
   - Select "LME Copper Analysis" workflow
   - Click "Run workflow"
   - Choose your options and run!

3. **View results**:
   - After workflow completes, download artifacts
   - Open \`copper_dashboard.html\` for interactive visualization

4. **Set up notifications** (optional):
   - Add email secrets for notifications
   - See [Secrets Setup Guide](../../tree/main/docs/SECRETS_SETUP.md)

### 📚 Documentation:

- [Workflows Guide](../../tree/main/docs/WORKFLOWS.md)
- [README](../../blob/main/README.md)

### 🤖 Automation:

The analysis will run automatically:
- Daily at 7:30 PM UTC
- Weekly on Mondays
- Monthly on the 1st

You can close this issue once you've reviewed the setup.

Happy analyzing! 📊`,
              labels: ['documentation', 'good first issue']
            });
            console.log(`Created setup completion issue #${issue.data.number}`);
